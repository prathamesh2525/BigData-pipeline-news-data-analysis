{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c267c93e-da2e-4b4a-aa03-2330c59bfd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql.functions import col, lower, regexp_replace\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"RobustWordCount\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9997b541-35af-434a-b9f5-0b3c2c5fd1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "\n",
    "    return spark.read.text(file_path)\n",
    "\n",
    "# Preprocessing: Remove punctuation, lowercase text, and remove stopwords\n",
    "def preprocess_text(df):\n",
    "\n",
    "    df = df.withColumn(\"cleaned_text\", regexp_replace(col(\"value\"), \"[^a-zA-Z\\\\s]\", \"\"))\n",
    "    \n",
    "\n",
    "    df = df.withColumn(\"cleaned_text\", lower(col(\"cleaned_text\")))\n",
    "\n",
    "\n",
    "    tokenizer = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"words\")\n",
    "    df = tokenizer.transform(df)\n",
    "    \n",
    "\n",
    "    remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "    df = remover.transform(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a42762f-38d7-4008-bc20-f5295fc7ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_frequencies(df):\n",
    "\n",
    "    word_counts = df.select(\"filtered_words\").rdd.flatMap(lambda row: row['filtered_words']) \\\n",
    "        .map(lambda word: (word, 1)) \\\n",
    "        .reduceByKey(lambda a, b: a + b)\n",
    "    \n",
    "    return word_counts\n",
    "\n",
    "# Function to save word count results to HDFS or any destination\n",
    "def save_word_counts(word_counts, output_path):\n",
    "\n",
    "    word_counts_df = word_counts.toDF([\"word\", \"count\"])\n",
    "    \n",
    "\n",
    "    word_counts_df.write.mode(\"overwrite\").csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f39e6ba-ab20-4f68-9209-d8d777b8fdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main pipeline function\n",
    "def main(file_path, output_path):\n",
    "    \n",
    "    df = read_data(file_path)\n",
    "    \n",
    "    preprocessed_df = preprocess_text(df)\n",
    "    \n",
    "    word_counts = count_word_frequencies(preprocessed_df)\n",
    "    \n",
    "    save_word_counts(word_counts, output_path)\n",
    "\n",
    "    print(f\"Word count results saved to: {output_path}\")\n",
    "\n",
    "\n",
    "input_file_path = \"hdfs://path/*.txt\"  # Input file path\n",
    "output_file_path = \"hdfs://path/word_counts\"  # Output directory for word counts\n",
    "\n",
    "# Call the main function to process the text\n",
    "main(input_file_path, output_file_path)\n",
    "\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
